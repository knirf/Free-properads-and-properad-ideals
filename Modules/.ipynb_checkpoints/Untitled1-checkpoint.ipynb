{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ideal_Properad:\n",
    "    '''\n",
    "    WIP!!!\n",
    "    Class for ideals in free properads, give the parent properad, the list of lists of elements generating the ideal, \n",
    "    a list of lists of coefficient for these elements and a name to save.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, prop, elements_init, name):\n",
    "        #Here we try to get a save for this ideal, if there is no save file, we create a new ideal via initiate.\n",
    "        self.prop = prop\n",
    "        self.elements_init = elements_init\n",
    "        self.name = name\n",
    "        try:\n",
    "            with open(\"Files/Ideals/\" + name, 'rb') as file:\n",
    "                ideal = pickle.load(file)\n",
    "                if type(ideal) == Ideal_Properad and ideal.elements_init == elements_init and ideal.name == self.name:\n",
    "                    self.elements = ideal.elements\n",
    "                    self.generated_steps = ideal.generated_steps\n",
    "                    print('Steps generated from file : ' + str(self.generated_steps))\n",
    "                else:\n",
    "                    if type(ideal) != Ideal_Properad:\n",
    "                        print('Type doesn\\'t match, generating new ideal.')\n",
    "                    if ideal.elements_init != elements_init:\n",
    "                        print('Initial elements don\\'t match, generating new ideal.')\n",
    "                    if ideal.elements_init != self.name:\n",
    "                        print('Name doesn\\'t match, generating new ideal.')\n",
    "                    self.initiate()\n",
    "        except FileNotFoundError as e:\n",
    "            print('Generating new ideal (error FilNotFoundError) : ' + self.name)\n",
    "            self.initiate()\n",
    "        except IOError as e:\n",
    "            print('Generating new ideal (error IOError) : ' + self.name)\n",
    "            self.initiate()\n",
    "        \n",
    "        \n",
    "    def initiate(self):\n",
    "        '''\n",
    "        Initiate the ideal if no backup has been found.\n",
    "        '''\n",
    "        self.elements = [elements_init]\n",
    "        self.generated_steps = [0]\n",
    "        \n",
    "    def is_homogeneous(self):\n",
    "        '''\n",
    "        Check if self is an homogeneous ideal. Doesn't work for now\n",
    "        '''\n",
    "        for element in self.elements_init:\n",
    "            first = element[0][0].weight\n",
    "            for el in element:\n",
    "                if el[0].weight != first:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    def generate_next_weight(self):\n",
    "        '''\n",
    "        Generate the new weight in self.\n",
    "        '''\n",
    "        result = [] # Final result, the list of all new elements in self.\n",
    "        for element in self.elements[-1]:\n",
    "            new_graphs = [] # This list will have as components one list by graph in element, which is the list of all new graphs from this one.\n",
    "            for graph in element:\n",
    "                coef = graph[1]\n",
    "                list_temp = []\n",
    "                for gen in self.prop.generators:\n",
    "                    list_temp += graph[0].do_all_links(gen)\n",
    "                new_graphs.append([[g, coef] for g in list_temp])\n",
    "            result += [[new_graphs[j][i] for j in range(len(new_graphs))] for i in range(len(new_graphs[0]))]\n",
    "        print('Generated all graphs')\n",
    "        try:\n",
    "            with open(\"Files/backup_ideal\", 'rb') as file:\n",
    "                backup = pickle.load(file)\n",
    "                if result != backup[0]:\n",
    "                    print(\"backup doesn't match, ereasing previous backup and starting a new one\")\n",
    "                    i_init = 0\n",
    "                    new_list = []\n",
    "                else:\n",
    "                    i_init = backup[1]\n",
    "                    new_list = backup[2]\n",
    "                    print(\"backup found, building from \" + str(i_init))\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"no backup found, building from start\")\n",
    "            i_init = 0\n",
    "            new_list = []\n",
    "        start = time.time()\n",
    "        l = len(result)\n",
    "        i = i_init\n",
    "        for element in result[i_init:]:\n",
    "            new_el = []\n",
    "            for graph in element:\n",
    "                for graph_prop in self.prop.by_weight[graph[0].weight]:\n",
    "                    if graph[0].compare(graph_prop):\n",
    "                        new_el.append([graph_prop.copy(), graph[1]])\n",
    "                        break\n",
    "            new_list.append(new_el)\n",
    "            timetemp = time.time()\n",
    "            i += 1\n",
    "            print(str(i) + \"/\" + str(l) + \" : took \" + str(int((timetemp - start)/60)) + \" minutes and \" + str(int(((timetemp - start)/60 - int((timetemp - start)/60))*60)) + \" seconds.\")\n",
    "            # We create a backup every 100 graphs\n",
    "            if i%100 == 0:\n",
    "                with open(\"Files/backup_ideal\", 'wb') as file:\n",
    "                    pickle.dump([result, i, new_list], file)\n",
    "                    print(\"created backup from i = \" + str(i))\n",
    "        print('Compared all graphs')\n",
    "        if os.path.isfile('Files/backup_ideal'):\n",
    "            os.remove('Files/backup_ideal')\n",
    "        self.elements.append(new_list)\n",
    "        self.generated_steps.append(self.generated_steps[-1]+1)\n",
    "        with open(\"Files/Ideals/\" + self.name, 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "            \n",
    "    def generate_next_weight_only_identities(self):\n",
    "        '''\n",
    "        Generate the new weight in self but keep only first graph of each element with identities above and below.\n",
    "        I still need to test this one but it should be way faster than the other one, but has a bit less information.\n",
    "        '''\n",
    "        result = [] # Final result, the list of all new elements in self.\n",
    "        for element in self.elements[-1]:\n",
    "            new_graphs = [] # This list will have as components one list by graph in element, which is the list of all new graphs from this one.\n",
    "            for graph in element:\n",
    "                coef = graph[1]\n",
    "                list_temp = []\n",
    "                for gen in self.prop.generators:\n",
    "                    list_temp += graph[0].do_all_links(gen)\n",
    "                new_graphs.append([[g, coef] for g in list_temp])\n",
    "            result += [[new_graphs[j][i] for j in range(len(new_graphs))] for i in range(len(new_graphs[0]))]\n",
    "        print('Generated all graphs')\n",
    "        try:\n",
    "            with open(\"Files/backup_ideal\", 'rb') as file:\n",
    "                backup = pickle.load(file)\n",
    "                if result != backup[0]:\n",
    "                    print(\"backup doesn't match, ereasing previous backup and starting a new one\")\n",
    "                    i_init = 0\n",
    "                    new_list = []\n",
    "                else:\n",
    "                    i_init = backup[1]\n",
    "                    new_list = backup[2]\n",
    "                    print(\"backup found, building from \" + str(i_init))\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"no backup found, building from start\")\n",
    "            i_init = 0\n",
    "            new_list = []\n",
    "        start = time.time()\n",
    "        l = len(result)\n",
    "        i = i_init\n",
    "        for element in result[i_init:]:\n",
    "            ref = element[0]\n",
    "            for graph_prop in self.prop.by_weight[ref[0].weight]:\n",
    "                if ref[0].compare(graph_prop):\n",
    "                    first_graph = [graph_prop.copy(), graph[1]]\n",
    "                    break\n",
    "            arity = first_graph[0].arity\n",
    "            if first_graph[0].perm_outputs == Permutations(arity[0])[0] and first_graph[0].perm_inputs == Permutations(arity[1])[1]:\n",
    "                new_el = [first_graph]\n",
    "                for graph in element[1:]:\n",
    "                    for graph_prop in self.prop.by_weight[graph[0].weight]:\n",
    "                        if graph[0].compare(graph_prop):\n",
    "                            new_el.append([graph_prop.copy(), graph[1]])\n",
    "                            break\n",
    "                new_list.append(new_el)\n",
    "            timetemp = time.time()\n",
    "            i += 1\n",
    "            print(str(i) + \"/\" + str(l) + \" : took \" + str(int((timetemp - start)/60)) + \" minutes and \" + str(int(((timetemp - start)/60 - int((timetemp - start)/60))*60)) + \" seconds.\")\n",
    "            # We create a backup every 100 graphs\n",
    "            if i%100 == 0:\n",
    "                with open(\"Files/backup_ideal\", 'wb') as file:\n",
    "                    pickle.dump([result, i, new_list], file)\n",
    "                    print(\"created backup from i = \" + str(i))\n",
    "        print('Compared all graphs')\n",
    "        if os.path.isfile('Files/backup_ideal'):\n",
    "            os.remove('Files/backup_ideal')\n",
    "        self.only_identities = new_list\n",
    "        with open(\"Files/Ideals/\" + self.name + \"_only_identities\", 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "        \n",
    "    def get_arity(self, step, arity):\n",
    "        '''\n",
    "        Return the list of elements of given step and arity.\n",
    "        '''\n",
    "        result = []\n",
    "        for element in self.elements[step]:\n",
    "            if element[0][0].arity == arity:\n",
    "                result.append(element)\n",
    "        return result\n",
    "    \n",
    "    def generate_arity(self, arity):\n",
    "        '''\n",
    "        Return the next step but only on given arity.\n",
    "        '''\n",
    "        result = [] # Final result, the list of all new elements in self.\n",
    "        for element in self.elements[-1]:\n",
    "            new_graphs = [] # This list will have as components one list by graph in element, which is the list of all new graphs from this one.\n",
    "            for graph in element:\n",
    "                coef = graph[1]\n",
    "                list_temp = []\n",
    "                for gen in self.prop.generators:\n",
    "                    list_temp += graph[0].do_all_links(gen, arity)\n",
    "                new_graphs.append([[g, coef] for g in list_temp])\n",
    "            result += [[new_graphs[j][i] for j in range(len(new_graphs))] for i in range(len(new_graphs[0]))]\n",
    "        print('Generated all graphs')\n",
    "        try:\n",
    "            with open(\"Files/backup_ideal\", 'rb') as file:\n",
    "                backup = pickle.load(file)\n",
    "                if result != backup[0]:\n",
    "                    print(\"backup doesn't match, ereasing previous backup and starting a new one\")\n",
    "                    i_init = 0\n",
    "                    new_list = []\n",
    "                else:\n",
    "                    i_init = backup[1]\n",
    "                    new_list = backup[2]\n",
    "                    print(\"backup found, building from \" + str(i_init))\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"no backup found, building from start\")\n",
    "            i_init = 0\n",
    "            new_list = []\n",
    "        start = time.time()\n",
    "        l = len(result)\n",
    "        i = i_init\n",
    "        for element in result[i_init:]:\n",
    "            new_el = []\n",
    "            for graph in element:\n",
    "                for graph_prop in self.prop.by_arity(graph[0].weight, arity):\n",
    "                    if graph[0].compare(graph_prop):\n",
    "                        new_el.append([graph_prop, graph[1]])\n",
    "                        break\n",
    "            new_list.append(new_el)\n",
    "            timetemp = time.time()\n",
    "            i += 1\n",
    "            print(str(i) + \"/\" + str(l) + \" : took \" + str(int((timetemp - start)/60)) + \" minutes and \" + str(int(((timetemp - start)/60 - int((timetemp - start)/60))*60)) + \" seconds.\")\n",
    "            # We create a backup every 100 graphs\n",
    "            if i%100 == 0:\n",
    "                with open(\"Files/backup_ideal\", 'wb') as file:\n",
    "                    pickle.dump([result, i, new_list], file)\n",
    "                    print(\"created backup from i = \" + str(i))\n",
    "        if os.path.isfile('Files/backup_ideal'):\n",
    "            os.remove('Files/backup_ideal')\n",
    "        print('Compared all graphs')\n",
    "        with open(\"Files/Ideals/\" + self.name + \"_arity\" + str(arity), 'wb') as file:\n",
    "            pickle.dump(new_list, file)\n",
    "        return new_list\n",
    "    \n",
    "    def generate_arity_only_identities(self, arity):\n",
    "        '''\n",
    "        Return the next step but only on given arity.\n",
    "        '''\n",
    "        result = [] # Final result, the list of all new elements in self.\n",
    "        for element in self.elements[-1]:\n",
    "            new_graphs = [] # This list will have as components one list by graph in element, which is the list of all new graphs from this one.\n",
    "            for graph in element:\n",
    "                coef = graph[1]\n",
    "                list_temp = []\n",
    "                for gen in self.prop.generators:\n",
    "                    list_temp += graph[0].do_all_links(gen, arity)\n",
    "                new_graphs.append([[g, coef] for g in list_temp])\n",
    "            result += [[new_graphs[j][i] for j in range(len(new_graphs))] for i in range(len(new_graphs[0]))]\n",
    "        print('Generated all graphs')\n",
    "        try:\n",
    "            with open(\"Files/backup_ideal\", 'rb') as file:\n",
    "                backup = pickle.load(file)\n",
    "                if result != backup[0]:\n",
    "                    print(\"backup doesn't match, ereasing previous backup and starting a new one\")\n",
    "                    i_init = 0\n",
    "                    new_list = []\n",
    "                else:\n",
    "                    i_init = backup[1]\n",
    "                    new_list = backup[2]\n",
    "                    print(\"backup found, building from \" + str(i_init))\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"no backup found, building from start\")\n",
    "            i_init = 0\n",
    "            new_list = []\n",
    "        start = time.time()\n",
    "        l = len(result)\n",
    "        i = i_init\n",
    "        for element in result[i_init:]:\n",
    "            ref = element[0]\n",
    "            for graph_prop in self.prop.by_weight[ref[0].weight]:\n",
    "                if ref[0].compare(graph_prop):\n",
    "                    first_graph = [graph_prop.copy(), ref[1]]\n",
    "                    break\n",
    "            arity = first_graph[0].arity\n",
    "            if first_graph[0].perm_outputs == Permutations(arity[0])[0] and first_graph[0].perm_inputs == Permutations(arity[1])[1]:\n",
    "                new_el = [first_graph]\n",
    "                for graph in element[1:]:\n",
    "                    for graph_prop in self.prop.by_weight[graph[0].weight]:\n",
    "                        if graph[0].compare(graph_prop):\n",
    "                            new_el.append([graph_prop.copy(), graph[1]])\n",
    "                            break\n",
    "                new_list.append(new_el)\n",
    "            timetemp = time.time()\n",
    "            i += 1\n",
    "            print(str(i) + \"/\" + str(l) + \" : took \" + str(int((timetemp - start)/60)) + \" minutes and \" + str(int(((timetemp - start)/60 - int((timetemp - start)/60))*60)) + \" seconds.\")\n",
    "            # We create a backup every 100 graphs\n",
    "            if i%100 == 0:\n",
    "                with open(\"Files/backup_ideal\", 'wb') as file:\n",
    "                    pickle.dump([result, i, new_list], file)\n",
    "                    print(\"created backup from i = \" + str(i))\n",
    "        if os.path.isfile('Files/backup_ideal'):\n",
    "            os.remove('Files/backup_ideal')\n",
    "        print('Compared all graphs')\n",
    "        with open(\"Files/Ideals/\" + self.name + \"_only_identities_arity\" + str(arity), 'wb') as file:\n",
    "            pickle.dump(new_list, file)\n",
    "        return new_list\n",
    "    \n",
    "    def sort_by_arity(self, step):\n",
    "        '''\n",
    "        Return a dic describing given step of self by arity. Keys : arities, items : list of graphs.\n",
    "        '''\n",
    "        dic = {}\n",
    "        for element in self.elements[step]:\n",
    "            first = element[0][0]\n",
    "            if first.arity not in dic:\n",
    "                dic[first.arity] = [element]\n",
    "            else:\n",
    "                dic[first.arity].append(element)\n",
    "        return dic\n",
    "    \n",
    "    def generate_matrices(self, step):\n",
    "        '''\n",
    "        Generate the matrices describing all elements of self in given step, one matrix by arity. \n",
    "        Returns a dic with arities as keys and matrices as items.\n",
    "        '''\n",
    "        dic = self.sort_by_arity(step)\n",
    "        matrix_spaces = {}\n",
    "        matrices = {}\n",
    "        for arity in dic:\n",
    "            rows = len(dic[arity])\n",
    "            columns = len(self.prop.by_arity(dic[arity][0][0][0].weight, arity))\n",
    "            matrix_spaces[arity] = MatrixSpace(Ring, rows, columns, sparse = is_sparse)\n",
    "            matrix = matrix_spaces[arity]()\n",
    "            for i in range(rows):\n",
    "                element = dic[arity][i]\n",
    "                for graph in element:\n",
    "                    matrix[i, self.prop.by_arity(graph[0].weight, arity).index(graph[0])] = graph[1]\n",
    "            matrices[arity] = matrix\n",
    "        return matrices\n",
    "    \n",
    "    def only_one_per_orbite(self, step):\n",
    "        '''\n",
    "        Return only the elements of the ideal with identity permutations on outputs and inputs.\n",
    "        '''\n",
    "        dic = {}\n",
    "        by_arity = self.sort_by_arity(step)\n",
    "        for arity in by_arity:\n",
    "            result = []\n",
    "            for element in by_arity[arity]:\n",
    "                if element[0][0].perm_outputs == Permutations(arity[0])[0] and element[0][0].perm_inputs == Permutations(arity[1])[0]:\n",
    "                    result.append(element)\n",
    "            dic[arity] = result\n",
    "        return dic\n",
    "        \n",
    "    def get_ids(self, step):\n",
    "        '''\n",
    "        Return a list of elements but instead of graphs we have ids of shapes and permutations.\n",
    "        '''\n",
    "        dic = {}\n",
    "        by_arity = self.only_one_per_orbite(step)\n",
    "        for arity in by_arity:\n",
    "            result = []\n",
    "            list_shapes = []\n",
    "            list_shapes_ext = []\n",
    "            for element in by_arity[arity]:\n",
    "                el_id = []\n",
    "                for graph in element:\n",
    "                    shape = graph[0].shape()\n",
    "                    if shape not in list_shapes:\n",
    "                        list_shapes_ext.append(graph[0])\n",
    "                        list_shapes.append(shape)\n",
    "                    el_id.append([[list_shapes.index(shape), graph[0].perm_outputs, graph[0].perm_inputs], graph[1]])\n",
    "                result.append(el_id)\n",
    "        #We potentially have some dups so we get rid of them.\n",
    "            keep = []\n",
    "            for el in result:\n",
    "                verif = True\n",
    "                for el2 in keep:\n",
    "                    if el == el2:\n",
    "                        verif = False\n",
    "                if verif:\n",
    "                    keep.append(el)\n",
    "            dic[arity] = [keep, list_shapes_ext]\n",
    "        return dic\n",
    "    \n",
    "    def get_Cliftons(self, step):\n",
    "        '''\n",
    "        Retuns a dic wich gives every Clifton matrices for self at given step. Keys : arities, items : dic.\n",
    "        Keys of subdics : couples of partitions, items : big matrix.\n",
    "        '''\n",
    "        \n",
    "        ids = self.get_ids(step)\n",
    "        result = {}\n",
    "        for arity in ids:\n",
    "            partitions_outputs = Partitions(arity[0])\n",
    "            partitions_inputs = Partitions(arity[1])\n",
    "            couples_partitions = []\n",
    "            for part_out in partitions_outputs:\n",
    "                for part_in in partitions_inputs:\n",
    "                    couples_partitions.append((part_out, part_in))\n",
    "            dic_matrices = {}\n",
    "            n = len(ids[arity][1]) #nb of cols\n",
    "            for couple in couples_partitions:\n",
    "                matrices = []\n",
    "                list_ids = ids[arity][0]\n",
    "                m = len(list_ids) #nb of rows\n",
    "                size_matrices = len(all_standard_tableaux_part(couple[0], arity[0])) * len(all_standard_tableaux_part(couple[1], arity[1]))\n",
    "                for element in list_ids:\n",
    "                    el_matrix_form = []\n",
    "                    for graph in element:\n",
    "                        mat = graph[1]*tensor_product(Clifton(couple[0], graph[0][1]), Clifton(couple[1], graph[0][2]))\n",
    "                        matrix_form = [graph[0][0], mat]\n",
    "                        el_matrix_form.append(matrix_form)\n",
    "                    matrices.append(el_matrix_form)\n",
    "                M = [[0 for i in range(n * size_matrices)] for j in range(m * size_matrices)]\n",
    "                for i in range(m):\n",
    "                    for j in range(n):\n",
    "                        el = matrices[i]\n",
    "                        for graph in el:\n",
    "                            if graph[0] == j:\n",
    "                                for k in range(size_matrices):\n",
    "                                    for l in range(size_matrices):\n",
    "                                        M[i*size_matrices + k][j*size_matrices + l] += graph[1][k, l]\n",
    "                                    \n",
    "                dic_matrices[couple] = Matrix(M)\n",
    "            result[arity] = dic_matrices\n",
    "        return result  \n",
    "    \n",
    "    def get_Cliftons_v2(self, step):\n",
    "        '''\n",
    "        Second shot... And it worked !\n",
    "        '''\n",
    "        \n",
    "        ids = self.get_ids(step)\n",
    "        result = {}\n",
    "        for arity in ids:\n",
    "            dic_matrix = {}\n",
    "            list_shapes = ids[arity][1]\n",
    "            list_rel = ids[arity][0]\n",
    "            partitions_outputs = Partitions(arity[0])\n",
    "            partitions_inputs = Partitions(arity[1])\n",
    "            couples_partitions = []\n",
    "            for part_out in partitions_outputs:\n",
    "                for part_in in partitions_inputs:\n",
    "                    couples_partitions.append((part_out, part_in))\n",
    "            #dic_matrices = {}\n",
    "            m = len(list_shapes) #nb of cols\n",
    "            n = len(list_rel) #nb of rows\n",
    "            for couple in couples_partitions:\n",
    "                size_matrices = len(all_standard_tableaux_part(couple[0], arity[0])) * len(all_standard_tableaux_part(couple[1], arity[1]))\n",
    "                Mat_couple = []\n",
    "                for i in range(m):\n",
    "                    shape = list_shapes[i]\n",
    "                    rowi = []\n",
    "                    for rel in list_rel:\n",
    "                        Mat_ij = Matrix([[0 for i in range(size_matrices)] for j in range(size_matrices)])\n",
    "                        for graph in rel:\n",
    "                            if graph[0][0] == i:\n",
    "                                Mat_ij = Mat_ij + graph[1] * tensor_product(Clifton(couple[0], graph[0][1]), Clifton(couple[1], graph[0][2]))\n",
    "                        rowi = rowi + [Mat_ij]\n",
    "                    Mat_couple = Mat_couple + [block_matrix([rowi]).transpose()]\n",
    "                Mat_couple = block_matrix([Mat_couple])\n",
    "                dic_matrix[couple] = Mat_couple\n",
    "            result[arity] = dic_matrix\n",
    "        return result\n",
    "    \n",
    "    def sort_by_arity_v2(self):\n",
    "        '''\n",
    "        Return a dic describing given step of self by arity. Keys : arities, items : list of graphs. This one is for self.only_identities.\n",
    "        '''\n",
    "        dic = {}\n",
    "        for element in self.only_identities:\n",
    "            first = element[0][0]\n",
    "            if first.arity not in dic:\n",
    "                dic[first.arity] = [element]\n",
    "            else:\n",
    "                dic[first.arity].append(element)\n",
    "        return dic\n",
    "    \n",
    "    def get_ids_v2(self):\n",
    "        '''\n",
    "        Return a list of elements but instead of graphs we have ids of shapes and permutations. This one is for self.only_identities.\n",
    "        '''\n",
    "        dic = {}\n",
    "        by_arity = self.by_arity_v2()\n",
    "        for arity in by_arity:\n",
    "            result = []\n",
    "            list_shapes = []\n",
    "            list_shapes_ext = []\n",
    "            for element in by_arity[arity]:\n",
    "                el_id = []\n",
    "                for graph in element:\n",
    "                    shape = graph[0].shape()\n",
    "                    if shape not in list_shapes:\n",
    "                        list_shapes_ext.append(graph[0])\n",
    "                        list_shapes.append(shape)\n",
    "                    el_id.append([[list_shapes.index(shape), graph[0].perm_outputs, graph[0].perm_inputs], graph[1]])\n",
    "                result.append(el_id)\n",
    "        #We potentially have some dups so we get rid of them.\n",
    "            keep = []\n",
    "            for el in result:\n",
    "                verif = True\n",
    "                for el2 in keep:\n",
    "                    if el == el2:\n",
    "                        verif = False\n",
    "                if verif:\n",
    "                    keep.append(el)\n",
    "            dic[arity] = [keep, list_shapes_ext]\n",
    "        return dic\n",
    "    \n",
    "    def get_Cliftons_v3(self, step):\n",
    "        '''\n",
    "        Second shot... And it worked ! This one is for self.only_identities.\n",
    "        '''\n",
    "        \n",
    "        ids = self.get_ids_v2(step)\n",
    "        result = {}\n",
    "        for arity in ids:\n",
    "            dic_matrix = {}\n",
    "            list_shapes = ids[arity][1]\n",
    "            list_rel = ids[arity][0]\n",
    "            partitions_outputs = Partitions(arity[0])\n",
    "            partitions_inputs = Partitions(arity[1])\n",
    "            couples_partitions = []\n",
    "            for part_out in partitions_outputs:\n",
    "                for part_in in partitions_inputs:\n",
    "                    couples_partitions.append((part_out, part_in))\n",
    "            #dic_matrices = {}\n",
    "            m = len(list_shapes) #nb of cols\n",
    "            n = len(list_rel) #nb of rows\n",
    "            for couple in couples_partitions:\n",
    "                size_matrices = len(all_standard_tableaux_part(couple[0], arity[0])) * len(all_standard_tableaux_part(couple[1], arity[1]))\n",
    "                Mat_couple = []\n",
    "                for i in range(m):\n",
    "                    shape = list_shapes[i]\n",
    "                    rowi = []\n",
    "                    for rel in list_rel:\n",
    "                        Mat_ij = Matrix([[0 for i in range(size_matrices)] for j in range(size_matrices)])\n",
    "                        for graph in rel:\n",
    "                            if graph[0][0] == i:\n",
    "                                Mat_ij = Mat_ij + graph[1] * tensor_product(Clifton(couple[0], graph[0][1]), Clifton(couple[1], graph[0][2]))\n",
    "                        rowi = rowi + [Mat_ij]\n",
    "                    Mat_couple = Mat_couple + [block_matrix([rowi]).transpose()]\n",
    "                Mat_couple = block_matrix([Mat_couple])\n",
    "                dic_matrix[couple] = Mat_couple\n",
    "            result[arity] = dic_matrix\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.3",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
